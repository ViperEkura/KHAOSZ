![image-20250306182014120](/resources/images/project_logo_clipped.png)

<div style="display: flex; flex-direction: column; align-items: center; justify-content: center; text-align: center; font-size: 16px; font-weight: bold; margin-top: 50px;">
  
  <div>
    <a href="#english" style="text-decoration: none; margin: 0 10px; color: blue;">English</a> | 
    <a href="#chinese" style="text-decoration: none; margin: 0 10px; color: blue;">ä¸­æ–‡</a>
  </div>

  <h1 style="margin: 20px 0 0 0; font-size: 2.5em; font-weight: bold;">KHAOSZ </h1>
</div>

<h2 id="english">English Version</h2>

This is a Chinese-English bilingual Transformer model supporting both languages. It contains model configurations and training workflows, completing training by loading parameters defined in `params/config.json`. The training script `train.py` parses command-line arguments, including dataset root directory, number of training epochs, batch size, checkpoint interval, and checkpoint directory.

**Model Download Options (Choose One):**

1. Visit [HuggingFace](https://huggingface.co/ViperEk/KHAOSZ) to access **Files and versions**
2. Run `params/download.py` to download parameters

**Demo Video:** [bilibili](https://www.bilibili.com/video/BV1z5RPYHEkd)

Training dataset sources are listed in the **Model Card** section of the HuggingFace download link.

**License:** Code follows Apache-2.0 protocol. Please credit the source code when used.

- **ğŸ“Š Device Selection:** Code defaults to CUDA training
- **ğŸŒ Performance Optimization:** `dtype=torch.bfloat16` is enabled to accelerate training and reduce memory usage. Ensure hardware supports this feature.
- **ğŸ¤– Language Support:** Model supports Chinese and English training. The BBPE tokenizer was trained without multilingual text, so OOV (out-of-vocabulary) issues are minimized for these languages but may exist for others.

### ğŸ“Œ Training Guide

To train this Transformer model, follow these steps:

**(1). Prepare Dataset:**

Place datasets in the designated root directory. Files should be text documents in Chinese, English, or mixed. Format should align with model input requirements - preferably pre-tokenized token_ids stored as `torch.Tensor` (using `torch.Tensor` saves memory compared to Python lists, which default to 64-bit precision).

**(2). Install Dependencies:**

```bash
pip install -r requirements.txt
pip install .
```

**(3). Run Training Script:**

```bash
python train.py \
--train_type=train_type[seq, sft, dpo] \
--data_root_path=/path/to/dataset \
--n_epoch=5 \
--batch_size=8 \
--max_lr=2e-4 \
--n_iter_ckpt=10000 \
--ckpt_dir checkpoints 
```

**Parameters Explanation:**
- `--train_type`: Training type (seq, sft, dpo)
- `--data_root_path`: Dataset root directory
- `--n_epoch`: Total training epochs
- `--batch_size`: Batch size
- `--n_iter_step`: Number of batches per training step
- `--warning_step`: Warmup steps
- `--max_lr`: Maximum learning rate (uses warmup + cosine decay)
- `--n_iter_ckpt`: Checkpoint saving interval
- `--ckpt_dir`: Checkpoint directory
- `--resume_dir`: Path to resume training from checkpoint

Training logs are saved in `train_log.txt`. Checkpoints will be stored in the specified directory for resuming training or evaluation.

### ğŸ‘‰ Usage Guide

**(1). Chatting with the Model:**

Open `chat.py` or use streaming/non-streaming interfaces:

**Streaming Output:**
```python
import torch
from khaosz import Khaosz

model_dir = "your_model_parameter_dir"
model = Khaosz(model_dir).to(device='cuda', dtype=torch.bfloat16)
history = []

while True:
    query = input(">> ")
    if query == "!exit":
        break
    
    response_size = 0
    for response, history in model.stream_generate(
        query=query, 
        history=history,
        temperature=0.85,
        top_p=0.95,
        top_k=50
    ):
        print(response[response_size:], end="")
        response_size = len(response)       
```

**Non-streaming Output:**
```python
import torch
from khaosz import Khaosz

model_dir = "your_model_parameter_dir"
model = Khaosz(model_dir).to(device='cuda', dtype=torch.bfloat16)
history = []

while True:
    query = input(">> ")
    if query == "!exit":
        break
    
    response = model.generate(
        query=query, 
        history=history,
        temperature=0.85,
        top_p=0.95,
        top_k=50
    )
    print(response)
```

**(2) Retrieval-Augmented Generation (RAG):**

```python
import torch
from khaosz import Khaosz

model_dir = "your_model_parameter_dir"
model = Khaosz(model_dir).to(device='cuda', dtype=torch.bfloat16)

retrieved_content = model.retrieve_generate(
    query=query,
    retrieve_top_k=5,
    temperature=0.6,
    top_k=30,
    top_p=0.95
)
print(retrieved_content)
```

### ğŸ“Œ Model Specifications

This model is based on a 24-layer Transformer with parameters defined in `config.json`, totaling approximately 1.0 billion (1.0B) parameters.

**Key Design Choices:**
- Weight tying between embedding and final linear layers (standard for small models to save parameters)
- Embedding layer optimization: Without weight tying, a 10,000-word vocabulary would consume ~102M parameters (0.1B)

**Limitations:**
- May struggle with complex language phenomena due to smaller parameter size
- Prone to overfitting on specialized datasets
- Limited multilingual capabilities

**Advantages:**
- Runs efficiently on lower-spec hardware
- Shorter training time compared to larger models

**Training Pipeline:** 
The model has completed pre-training + SFT (Supervised Fine-Tuning) + DPO (Direct Preference Optimization) workflows. All corresponding training code is included in the repository.


<h2 id="chinese">ä¸­æ–‡ç‰ˆæœ¬</h2>
è¿™æ˜¯ä¸€ä¸ªæ”¯æŒä¸­è‹±æ–‡åŒè¯­çš„ Transformer æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†ä¸¤ç§è¯­è¨€ã€‚æ¨¡å‹åŒ…å«é…ç½®æ–‡ä»¶å’Œè®­ç»ƒæµç¨‹ï¼Œé€šè¿‡åŠ è½½ `params/config.json` ä¸­å®šä¹‰çš„å‚æ•°å®Œæˆè®­ç»ƒã€‚è®­ç»ƒè„šæœ¬ `train.py` æ”¯æŒå‘½ä»¤è¡Œå‚æ•°è§£æï¼ŒåŒ…æ‹¬æ•°æ®é›†æ ¹ç›®å½•ã€è®­ç»ƒè½®æ•°ï¼ˆepochsï¼‰ã€æ‰¹é‡å¤§å°ï¼ˆbatch sizeï¼‰ã€æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”ã€æ£€æŸ¥ç‚¹ç›®å½•ç­‰ã€‚

**æ¨¡å‹ä¸‹è½½é€‰é¡¹ï¼ˆä»»é€‰å…¶ä¸€ï¼‰ï¼š**

1. è®¿é—® [HuggingFace](https://huggingface.co/ViperEk/KHAOSZ) æŸ¥çœ‹ **Files and versions**
2. è¿è¡Œ `params/download.py` ä¸‹è½½æ¨¡å‹å‚æ•°

**æ¼”ç¤ºè§†é¢‘ï¼š** [bilibili](https://www.bilibili.com/video/BV1z5RPYHEkd)

è®­ç»ƒæ•°æ®æ¥æºè¯·å‚è§ HuggingFace ä¸‹è½½é¡µé¢ä¸­çš„ **Model Card** éƒ¨åˆ†ã€‚

**è®¸å¯è¯ï¼š** ä»£ç éµå¾ª Apache-2.0 åè®®ï¼Œä½¿ç”¨æ—¶è¯·æ³¨æ˜å‡ºå¤„ã€‚

- **ğŸ“Š è®¾å¤‡é€‰æ‹©ï¼š** é»˜è®¤ä½¿ç”¨ CUDA è¿›è¡Œè®­ç»ƒ
- **ğŸŒ æ€§èƒ½ä¼˜åŒ–ï¼š** å¯ç”¨ `dtype=torch.bfloat16` ä»¥åŠ é€Ÿè®­ç»ƒå¹¶å‡å°‘å†…å­˜å ç”¨ï¼Œè¯·ç¡®ä¿ç¡¬ä»¶æ”¯æŒè¯¥ç‰¹æ€§
- **ğŸ¤– è¯­è¨€æ”¯æŒï¼š** æ¨¡å‹æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡è®­ç»ƒã€‚ç”±äº BBPE åˆ†è¯å™¨æœªä½¿ç”¨å¤šè¯­è¨€æ–‡æœ¬è®­ç»ƒï¼Œå› æ­¤ä¸­è‹±æ–‡çš„ OOVï¼ˆæœªç™»å½•è¯ï¼‰é—®é¢˜è¾ƒå°‘ï¼Œå…¶ä»–è¯­è¨€å¯èƒ½å­˜åœ¨ OOV é—®é¢˜



### ğŸ“Œ è®­ç»ƒæŒ‡å—

è¦è®­ç»ƒè¯¥ Transformer æ¨¡å‹ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š

#### **(1). å‡†å¤‡æ•°æ®é›†ï¼š**

å°†æ•°æ®é›†æ”¾ç½®åœ¨æŒ‡å®šçš„æ ¹ç›®å½•ä¸‹ã€‚æ–‡ä»¶åº”ä¸ºåŒ…å«ä¸­æ–‡ã€è‹±æ–‡æˆ–æ··åˆæ–‡æœ¬çš„æ–‡æœ¬æ–‡æ¡£ã€‚æ ¼å¼åº”ç¬¦åˆæ¨¡å‹è¾“å…¥è¦æ±‚â€”â€”å»ºè®®ä½¿ç”¨é¢„åˆ†è¯åçš„ `token_ids` å¹¶ä»¥ `torch.Tensor` æ ¼å¼ä¿å­˜ï¼ˆä½¿ç”¨ `torch.Tensor` ç›¸æ¯” Python åˆ—è¡¨æ›´èŠ‚çœå†…å­˜ï¼Œåˆ—è¡¨é»˜è®¤ä¸º 64 ä½ç²¾åº¦ï¼‰ã€‚

#### **(2). å®‰è£…ä¾èµ–ï¼š**

```bash
pip install -r requirements.txt
pip install .
```

#### **(3). è¿è¡Œè®­ç»ƒè„šæœ¬ï¼š**

```bash
python train.py \
--train_type=train_type[seq, sft, dpo] \
--data_root_path=/path/to/dataset \
--n_epoch=5 \
--batch_size=8 \
--max_lr=2e-4 \
--n_iter_ckpt=10000 \
--ckpt_dir checkpoints 
```

**å‚æ•°è¯´æ˜ï¼š**
- `--train_type`: è®­ç»ƒç±»å‹ï¼ˆseq, sft, dpoï¼‰
- `--data_root_path`: æ•°æ®é›†æ ¹ç›®å½•
- `--n_epoch`: æ€»è®­ç»ƒè½®æ•°
- `--batch_size`: æ‰¹é‡å¤§å°
- `--n_iter_step`: æ¯ä¸ªè®­ç»ƒæ­¥éª¤çš„ batch æ•°é‡
- `--warning_step`: é¢„çƒ­æ­¥æ•°ï¼ˆwarmup stepsï¼‰
- `--max_lr`: æœ€å¤§å­¦ä¹ ç‡ï¼ˆä½¿ç”¨é¢„çƒ­ + ä½™å¼¦è¡°å‡ï¼‰
- `--n_iter_ckpt`: æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”
- `--ckpt_dir`: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
- `--resume_dir`: ä»æŒ‡å®šè·¯å¾„æ¢å¤è®­ç»ƒ

è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åœ¨ `train_log.txt` ä¸­ã€‚æ£€æŸ¥ç‚¹å°†ä¿å­˜åœ¨æŒ‡å®šç›®å½•ï¼Œç”¨äºæ¢å¤è®­ç»ƒæˆ–è¯„ä¼°ã€‚



### ğŸ‘‰ ä½¿ç”¨æŒ‡å—

#### **(1). ä¸æ¨¡å‹å¯¹è¯ï¼š**

æ‰“å¼€ `chat.py` æˆ–ä½¿ç”¨æµå¼/éæµå¼æ¥å£ï¼š

**æµå¼è¾“å‡ºï¼š**
```python
import torch
from khaosz import Khaosz

model_dir = "your_model_parameter_dir"
model = Khaosz(model_dir).to(device='cuda', dtype=torch.bfloat16)
history = []

while True:
    query = input(">> ")
    if query == "!exit":
        break
    
    response_size = 0
    for response, history in model.stream_generate(
        query=query, 
        history=history,
        temperature=0.85,
        top_p=0.95,
        top_k=50
    ):
        print(response[response_size:], end="")
        response_size = len(response)       
```

**éæµå¼è¾“å‡ºï¼š**
```python
import torch
from khaosz import Khaosz

model_dir = "your_model_parameter_dir"
model = Khaosz(model_dir).to(device='cuda', dtype=torch.bfloat16)
history = []

while True:
    query = input(">> ")
    if query == "!exit":
        break
    
    response = model.generate(
        query=query, 
        history=history,
        temperature=0.85,
        top_p=0.95,
        top_k=50
    )
    print(response)
```

#### **(2). åŸºäºæ£€ç´¢çš„ç”Ÿæˆï¼ˆRAGï¼‰ï¼š**

```python
import torch
from khaosz import Khaosz

model_dir = "your_model_parameter_dir"
model = Khaosz(model_dir).to(device='cuda', dtype=torch.bfloat16)

retrieved_content = model.retrieve_generate(
    query=query,
    retrieve_top_k=5,
    temperature=0.6,
    top_k=30,
    top_p=0.95
)
print(retrieved_content)
```



### ğŸ“Œ æ¨¡å‹è§„æ ¼è¯´æ˜ï¼ˆé‡å¤éƒ¨åˆ†ï¼‰

è¯¥æ¨¡å‹åŸºäºä¸€ä¸ª 24 å±‚çš„ Transformer æ¶æ„ï¼Œå‚æ•°é…ç½®å®šä¹‰åœ¨ `config.json` ä¸­ï¼Œæ€»å‚æ•°é‡çº¦ä¸º 10 äº¿ï¼ˆ1.0Bï¼‰ã€‚

**å…³é”®è®¾è®¡é€‰æ‹©ï¼š**
- åœ¨åµŒå…¥å±‚ï¼ˆembeddingï¼‰ä¸æœ€ç»ˆçº¿æ€§å±‚ä¹‹é—´è¿›è¡Œæƒé‡ç»‘å®šï¼ˆweight tyingï¼‰ï¼Œè¿™æ˜¯å°å‹æ¨¡å‹ä¸­å¸¸è§çš„èŠ‚çœå‚æ•°é‡çš„åšæ³•
- åµŒå…¥å±‚ä¼˜åŒ–ï¼šè‹¥ä¸è¿›è¡Œæƒé‡ç»‘å®šï¼Œä¸€ä¸ªåŒ…å« 10,000 ä¸ªè¯çš„è¯æ±‡è¡¨å°†æ¶ˆè€—çº¦ 1.02 äº¿ï¼ˆ0.1Bï¼‰å‚æ•°

**å±€é™æ€§ï¼š**
- ç”±äºå‚æ•°è§„æ¨¡è¾ƒå°ï¼Œå¯èƒ½åœ¨å¤„ç†å¤æ‚è¯­è¨€ç°è±¡æ—¶è¡¨ç°å—é™
- åœ¨ç‰¹å®šé¢†åŸŸçš„æ•°æ®é›†ä¸Šå®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆ
- å¤šè¯­è¨€èƒ½åŠ›æœ‰é™

**ä¼˜åŠ¿ï¼š**
- å¯åœ¨ä½é…ç½®ç¡¬ä»¶ä¸Šé«˜æ•ˆè¿è¡Œ
- ç›¸è¾ƒäºå¤§å‹æ¨¡å‹ï¼Œè®­ç»ƒæ—¶é—´æ›´çŸ­

**è®­ç»ƒæµç¨‹ï¼š**  
è¯¥æ¨¡å‹å·²å®Œæˆé¢„è®­ç»ƒï¼ˆpre-trainingï¼‰+ ç›‘ç£å¾®è°ƒï¼ˆSFT, Supervised Fine-Tuningï¼‰+ ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPO, Direct Preference Optimizationï¼‰çš„å…¨æµç¨‹ã€‚æ‰€æœ‰ç›¸å…³çš„è®­ç»ƒä»£ç å‡å·²åŒ…å«åœ¨ä»£ç åº“ä¸­ã€‚